{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Res Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision import transforms  \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = Compose([\n",
    "     transforms.ConvertImageDtype(torch.float),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "     transforms.Pad(1)\n",
    " ])\n",
    "\n",
    "class CustomCountriesDataset(Dataset):\n",
    "    def __init__(self, df, transform=tf, target_transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.df.iloc[idx, 0])\n",
    "        label = self.df.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../output.csv\")\n",
    "\n",
    "X = df['images']\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomCountriesDataset(train_data, transform=tf)\n",
    "test_dataset = CustomCountriesDataset(test_data, transform=tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_features, train_labels = next(iter(train_dataloader))\\nprint(f\"Feature batch shape: {train_features}\")\\nprint(f\"Labels batch shape: {train_labels}\")\\nimg = train_features[0].squeeze()\\nlabel = train_labels[0]\\nimage_np = img.numpy().transpose((1, 2, 0))\\nplt.imshow(image_np)\\nplt.show()\\nprint(f\"Label: {label}\")'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsz = 1\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bsz, shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bsz, shuffle=False)\n",
    "\n",
    "#print(next(iter(train_dataloader)))\n",
    "\"\"\"\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features}\")\n",
    "print(f\"Labels batch shape: {train_labels}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "image_np = img.numpy().transpose((1, 2, 0))\n",
    "plt.imshow(image_np)\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels=6, kernel_size=3, stride=1, padding=1), # 224\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(6, 22, kernel_size=3, stride=1, padding=1), # 112\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(22, 76, kernel_size=3, stride=1, padding=1), # 56\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(76, 120, kernel_size=3, stride=1, padding=1), # 28\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(120, 200, kernel_size=3, stride=1, padding=1), # 14\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(200, 220, kernel_size=3, stride=1, padding=1), # 7\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # 3\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # FCL\n",
    "            nn.Linear(220 * 3 * 3, 120),\n",
    "            nn.Linear(120, 4),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        #pass\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/38895 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 Loss: 1.4839 Acc: 0.2599:  11%|â–ˆ         | 4207/38895 [00:33<04:38, 124.74batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# backprop\u001b[39;00m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m j \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\artui\\miniforge3\\envs\\gg\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\artui\\miniforge3\\envs\\gg\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CNN(3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    j = 0\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batches\")\n",
    "    for i, (im, cl) in enumerate(bar):\n",
    "        im = im.to(device)\n",
    "        cl = cl.to(device)\n",
    "        # forward pass\n",
    "        pred = model(im)\n",
    "        loss = criterion(pred, cl)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        j += 1\n",
    "\n",
    "        # loss calc\n",
    "        running_loss += loss.item()\n",
    "        total_samples += cl.size(0)\n",
    "\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct_predictions += (predicted == cl).sum().item()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            bar.set_description(f\"Epoch {epoch+1}/{num_epochs} Loss: {running_loss / total_samples:.4f} Acc: {correct_predictions / total_samples:.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
